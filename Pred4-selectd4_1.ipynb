{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with LSTM model (v.03)\n",
    "<p style='text-align: right;'>with selectd4.csv</p>\n",
    "\n",
    "* history\n",
    "  * 2025/05/23 PM06:02 : 3번째 모델의 3번째 데이터를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "from datetime import datetime\n",
    "print(f'restart kernel... {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restart kernel... 2025-05-27 15:02:25.226059\n"
     ]
    }
   ],
   "source": [
    "def restart_kernel():\n",
    "    # Restart the kernet after libraries are loaded.\n",
    "    import IPython\n",
    "    from datetime import datetime\n",
    "    print(f'restart kernel... {datetime.now()}')\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported library. (2025-05-27 15:04:02.062023)\n"
     ]
    }
   ],
   "source": [
    "# load dependacies\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from lib.globalvar import *\n",
    "from lib.data_loader import DataLoader\n",
    "from lib.model1 import PredictLSTM1\n",
    "from lib.model2 import PredictLSTM2\n",
    "from lib.model3 import PredictLSTM3\n",
    "from lib.model4 import PredictLSTM4\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.util_pred import print_data\n",
    "from lib.util_pred import flat_data_with_sum, flat_data, get_frequency\n",
    "from lib.util_pred import save_model, import_mode\n",
    "from lib.util_pred import get_random_in_list\n",
    "from lib.util_pred import print_data_with_sort\n",
    "from lib.util_pred import get_sorted_n_values\n",
    "from lib.util_pred import dict_key_count\n",
    "from lib.util_pred import print_list\n",
    "from lib.util_pred import print_dict_list\n",
    "from lib.util_pred import print_title\n",
    "from lib.util_pred import listinlist_2_strinlist\n",
    "from lib.util_pred import change_matched_info\n",
    "from lib.json_util import write_json\n",
    "\n",
    "from lib.small_predict import predict_small, print_predict_small\n",
    "\n",
    "from lib.activation import ActivationOutput, RecurrentActivation\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" db \"\"\"\n",
    "from lib.db_operate import conn_db, close_db\n",
    "from lib.db_operate import init_metric, init_models\n",
    "from lib.db_operate import select_metric, delete_all_metric\n",
    "from lib.db_operate import insert_metric\n",
    "from lib.db_operate import select_model, insert_model\n",
    "\n",
    "from lib.operate import create_model, create_model_v1, get_predicted, create_model_v2\n",
    "\n",
    "print(f\"imported library. ({datetime.now()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished to set environemnt. (2025-05-27 15:02:35.220407)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## 모델링 환경 설정\n",
    "##\n",
    "window=1 # default = 1 , help = \"time stamps\"\n",
    "data_dir='./csv/selectd4.csv'\n",
    "    \n",
    "mode='predict' # help = \"back-test or predict\")\n",
    "\n",
    "mode2='sampling' # help = \"greed or sampling\")\n",
    "verb='verbose' # help = \"verbose or not_verb\")\n",
    "    \n",
    "trial=20 # help = \"how much trials to generate\")\n",
    "training_length=0.95 # default = 0.9)\n",
    "epoch=20 # default = 3\n",
    "batch=1 # default = 1\n",
    "model_type='lstm4' # help = \"lstm1 or lstm2\")\n",
    "hid_dim = 50\n",
    "from_pos = 0\n",
    "last = [[1, 5, 18, 20, 30, 35],\n",
    "        [7, 9, 24, 40, 42, 44],\n",
    "        [3, 6, 7, 11, 12, 17],\n",
    "        [3, 13, 28, 34, 38, 42],\n",
    "        [5, 12, 24, 29, 32, 42]]\n",
    "print(f\"finished to set environemnt. ({datetime.now()})\")\n",
    "MAX_MODEL_CNT = 6\n",
    "\n",
    "write_to_db = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from '/home/swhors/jupyter-workspace/finance/venv/lib/python3.11/site-packages/tensorflow/_api/v2/version/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f'{tf.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed to load data. 2025-05-27 15:04:19.807867\n"
     ]
    }
   ],
   "source": [
    "dataset_dicts = {\n",
    "    1: DataLoader(data_dir=data_dir,\n",
    "                  training_length=training_length,\n",
    "                  window_prev=window,\n",
    "                  mode=mode,\n",
    "                  from_pos=0,\n",
    "                  verbose=False\n",
    "                  ),\n",
    "    2: DataLoader(data_dir=data_dir,\n",
    "                  training_length=training_length,\n",
    "                  window_prev=window,\n",
    "                  mode=mode,\n",
    "                  from_pos=300,\n",
    "                  verbose=False\n",
    "                  )\n",
    "    }\n",
    "#print(f'test_X = {dataset_dicts[1].test_X}')\n",
    "print(f'completed to load data. {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed to set evn for all models. 2025-05-27 15:52:58.883334\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "## layers\n",
    "## LSTM Neural 계층 선언\n",
    "###########\n",
    "\"\"\"\n",
    "Activation (Output):\n",
    "    linear: No activation, output is directly passed through.\n",
    "    relu: Rectified Linear Unit, max(x, 0).\n",
    "    sigmoid: Sigmoid function, output between 0 and 1.\n",
    "    tanh: Hyperbolic tangent, output between -1 and 1.\n",
    "    softmax: Normalizes output to a probability distribution.\n",
    "    elu: Exponential Linear Unit.\n",
    "    selu: Scaled Exponential Linear Unit.\n",
    "\n",
    "Recurrent Activation:\n",
    "    sigmoid: Commonly used for gates in LSTM.\n",
    "    hard_sigmoid: A faster, less computationally expensive version of sigmoid.\n",
    "    tanh: Can be used, but sigmoid is more typical for gates.\n",
    "\"\"\"\n",
    "datasets = [dataset_dicts[1], # 0\n",
    "            dataset_dicts[2], # 1\n",
    "           ]\n",
    "\n",
    "lstm_units=[[(45, ActivationOutput.selu.name),   # 0\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.sigmoid.name),\n",
    "             (45, ActivationOutput.elu.name)\n",
    "             ],\n",
    "            [(45, ActivationOutput.selu.name),   # 1\n",
    "             (45, ActivationOutput.selu.name)\n",
    "             ],\n",
    "            [(45, ActivationOutput.selu.name),   # 2\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.selu.name),\n",
    "             (45, ActivationOutput.sigmoid.name),\n",
    "             (45, ActivationOutput.sigmoid.name),\n",
    "             (45, ActivationOutput.elu.name)\n",
    "             ],\n",
    "            [(45, ActivationOutput.tanh.name),   # 3\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             (45, ActivationOutput.tanh.name)\n",
    "             ],\n",
    "            [(45, ActivationOutput.tanh.name),   # 4\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             (45, ActivationOutput.tanh.name),\n",
    "             ],\n",
    "            [(45, ActivationOutput.tanh.name)    # 5\n",
    "             ],\n",
    "            ]\n",
    "dense_units = [[(45, ActivationOutput.selu.name),   # 0\n",
    "                (45, ActivationOutput.elu.name),\n",
    "                (45, ActivationOutput.elu.name),],\n",
    "               [(45, ActivationOutput.selu.name),], # 1\n",
    "               [(45, ActivationOutput.selu.name),   # 2\n",
    "                (45, ActivationOutput.elu.name),\n",
    "                (45, ActivationOutput.elu.name),],\n",
    "               [(45, ActivationOutput.tanh.name),   # 3\n",
    "               ],\n",
    "               []                                   # 4\n",
    "              ]\n",
    "sel_date = datetime(2025, 5, 24, 20, 35)\n",
    "sel_date_ts = (sel_date.timestamp() / 10000000)\n",
    "steps = [45, 45,]\n",
    "metrics = [[\"accuracy\"]]\n",
    "dropout = [0, 0]\n",
    "learning_rate = [0.01, 0.05]\n",
    "last_lstm_return_sequences = [False]\n",
    "loss = [\"binary_crossentropy\", \"mse\"]\n",
    "output_dense_activation = [ActivationOutput.elu.name, #0\n",
    "                           ActivationOutput.selu.name, #1\n",
    "                           ActivationOutput.sigmoid.name, #2\n",
    "                           ActivationOutput.tanh.name, #3\n",
    "                           ActivationOutput.softmax.name, #4\n",
    "                           ActivationOutput.relu.name, #5\n",
    "                           ActivationOutput.linear.name #6\n",
    "                          ]\n",
    "epochs = [10, #0\n",
    "          20, #1\n",
    "          25, #2\n",
    "          50, #3\n",
    "          100] #4\n",
    "\n",
    "rand_seed=[sel_date_ts, datetime.now().timestamp(), 0]\n",
    "\n",
    "\n",
    "lstm_args = {1: {\"model\":{\"learning_rate\": learning_rate[0],\n",
    "                          \"dropout\": dropout[0],\n",
    "                          \"steps\": steps[0], # last output elements count\n",
    "                          \"metrics\": metrics[0],\n",
    "                          \"last_lstm_return_sequences\": last_lstm_return_sequences[0],\n",
    "                          \"lstm_units\": lstm_units[0],\n",
    "                          \"dense_units\": dense_units[0],\n",
    "                          \"output_dense_activation\": output_dense_activation[0],\n",
    "                          \"loss\": loss[0],\n",
    "                          \"rand_seed\": rand_seed[0]\n",
    "                          },\n",
    "                 \"train\": {\"epochs\": epochs[3],\n",
    "                           \"is_shuffle\": False,\n",
    "                           \"batch\": 1,\n",
    "                           \"steps_per_epoch\": 1\n",
    "                           },\n",
    "                 \"dataset\": datasets[0]\n",
    "                 },\n",
    "             2: {\"model\":{\"learning_rate\": learning_rate[0],\n",
    "                          \"dropout\": dropout[0],\n",
    "                          \"steps\": steps[0], # last output elements count\n",
    "                          \"metrics\": metrics[0],\n",
    "                          \"last_lstm_return_sequences\": last_lstm_return_sequences[0],\n",
    "                          \"lstm_units\": lstm_units[1],\n",
    "                          \"dense_units\": dense_units[1],\n",
    "                          \"output_dense_activation\": output_dense_activation[0],\n",
    "                          \"loss\": loss[0],\n",
    "                          \"rand_seed\": rand_seed[0]\n",
    "                          },\n",
    "                 \"train\": {\"epochs\": epochs[1],\n",
    "                           \"is_shuffle\": False,\n",
    "                           \"batch\": 1,\n",
    "                           \"steps_per_epoch\": 1\n",
    "                           },\n",
    "                 \"dataset\": datasets[0]\n",
    "                 },\n",
    "             3: {\"model\":{\"learning_rate\": learning_rate[0],\n",
    "                          \"dropout\": dropout[0],\n",
    "                          \"steps\": steps[0], # last output elements count\n",
    "                          \"metrics\": metrics[0],\n",
    "                          \"last_lstm_return_sequences\": last_lstm_return_sequences[0],\n",
    "                          \"lstm_units\": lstm_units[1],\n",
    "                          \"dense_units\": dense_units[1],\n",
    "                          \"output_dense_activation\": output_dense_activation[0],\n",
    "                          \"loss\": loss[0],\n",
    "                          \"rand_seed\": rand_seed[1]\n",
    "                          },\n",
    "                 \"train\": {\"epochs\": epochs[3],\n",
    "                           \"is_shuffle\": False,\n",
    "                           \"batch\": 1,\n",
    "                           \"steps_per_epoch\": 1\n",
    "                           },\n",
    "                 \"dataset\": datasets[0]\n",
    "                 },\n",
    "             4: {\"model\":{\"learning_rate\": learning_rate[0],\n",
    "                          \"dropout\": dropout[1],\n",
    "                          \"steps\": steps[0], # last output elements count\n",
    "                          \"metrics\": metrics[0],\n",
    "                          \"last_lstm_return_sequences\": last_lstm_return_sequences[0],\n",
    "                          \"lstm_units\": lstm_units[2],\n",
    "                          \"dense_units\": dense_units[2],\n",
    "                          \"output_dense_activation\": output_dense_activation[3],\n",
    "                          \"loss\": loss[1],\n",
    "                          \"return_state\": False,\n",
    "                          \"rand_seed\": rand_seed[0] # rand_seed[0]\n",
    "                          },\n",
    "                 \"train\": {\"epochs\": epochs[3],\n",
    "                           \"is_shuffle\": False,\n",
    "                           \"batch\": 3,\n",
    "                           \"steps_per_epoch\": 90 #100\n",
    "                           },\n",
    "                 \"dataset\": datasets[0]\n",
    "                 },\n",
    "             5: {\"model\":{\"learning_rate\": learning_rate[0],\n",
    "                          \"dropout\": dropout[1],\n",
    "                          \"steps\": steps[0], # last output elements count\n",
    "                          \"metrics\": metrics[0],\n",
    "                          \"last_lstm_return_sequences\": last_lstm_return_sequences[0],\n",
    "                          \"lstm_units\": lstm_units[3],\n",
    "                          \"dense_units\": dense_units[3],\n",
    "                          \"output_dense_activation\": output_dense_activation[3],\n",
    "                          \"loss\": loss[1],\n",
    "                          \"rand_seed\": rand_seed[0]\n",
    "                          },\n",
    "                 \"train\": {\"epochs\": epochs[3],\n",
    "                           \"is_shuffle\": False,\n",
    "                           \"batch\": 3,\n",
    "                           \"steps_per_epoch\": 100\n",
    "                           },\n",
    "                 \"dataset\": datasets[0]\n",
    "                 },\n",
    "             6: {\"model\":{\"learning_rate\": learning_rate[0],\n",
    "                          \"dropout\": dropout[1],\n",
    "                          \"steps\": steps[0], # last output elements count\n",
    "                          \"metrics\": metrics[0],\n",
    "                          \"last_lstm_return_sequences\": last_lstm_return_sequences[0],\n",
    "                          \"lstm_units\": lstm_units[5],\n",
    "                          \"dense_units\": dense_units[4],\n",
    "                          \"output_dense_activation\": output_dense_activation[3],\n",
    "                          \"loss\": loss[1],\n",
    "                          \"return_state\": False,\n",
    "                          \"lstm_model\": \"Bidirectional\",\n",
    "                          \"rand_seed\": rand_seed[0] # rand_seed[0]\n",
    "                          },\n",
    "                 \"train\": {\"epochs\": epochs[3],\n",
    "                           \"is_shuffle\": False,\n",
    "                           \"batch\": 3,\n",
    "                           \"steps_per_epoch\": 90 #100\n",
    "                           },\n",
    "                 \"dataset\": datasets[0]\n",
    "                 }\n",
    "             }\n",
    "\n",
    "\n",
    "models = [None for i in range(MAX_MODEL_CNT)]\n",
    "\n",
    "matched_cnts = {}\n",
    "selected_fives = {}\n",
    "matched_list = {}\n",
    "predicted_all = {}\n",
    "\n",
    "print(f'completed to set evn for all models. {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run = False\n",
    "model_id=5\n",
    "if single_run:\n",
    "    model_version = 0\n",
    "    with open('active.version', 'r') as fd:\n",
    "        line = fd.read()\n",
    "        model_version = int(line)\n",
    "        print(f'previous model_version={model_version}')\n",
    "        print(f'open model version file {datetime.now()}')\n",
    "        \n",
    "        # single : create model and training\n",
    "        print(f'start to train all models. {datetime.now()}')\n",
    "        print(f'{model_id+1}\\'s training. status=start {datetime.now()}')\n",
    "        model = create_model_v2(id=model_id,\n",
    "                            model_type=\"lstm4\",\n",
    "                            lstm_info=lstm_args[model_id+1],\n",
    "                            dataset=lstm_args[model_id+1][\"dataset\"],\n",
    "                            verbose=False)\n",
    "        # models[model_id] = (model, lstm_args[model_id+1])\n",
    "        print(f'{model_id+1}\\'s training. status=end {datetime.now()}')\n",
    "        print(f'completed to train models. {datetime.now()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(test_id, version, models, verbose=False):\n",
    "    from lib.db_operate import insert_model\n",
    "    import pickle\n",
    "    conn = conn_db('./db/metrics.db')\n",
    "    init_models(conn)\n",
    "    model_id = 1\n",
    "    date = datetime.now()\n",
    "    for model in models:\n",
    "        model_bin = pickle.dumps(model[0].model)\n",
    "        insert_model(conn=conn,\n",
    "                     test_id=test_id,\n",
    "                     version=version,\n",
    "                     model_id=model_id,\n",
    "                     date=date,\n",
    "                     model=model_bin,\n",
    "                     verbose=verbose\n",
    "                    )\n",
    "        model_id += 1\n",
    "    close_db(conn)\n",
    "\n",
    "def gen_multi_model(test_id, lstm_args, version, max_model_cnt, verbose=False):\n",
    "    \"\"\" gen_multi_model \"\"\"\n",
    "    models = []\n",
    "    print(f'start to train all models. {datetime.now()}')\n",
    "    for i in range(max_model_cnt):\n",
    "        print(f'{i+1}\\'s training. status=start {datetime.now()}')\n",
    "        model = create_model_v2(id=i,\n",
    "                                model_type=\"lstm4\",\n",
    "                                lstm_info=lstm_args[i+1],\n",
    "                                dataset=lstm_args[i+1][\"dataset\"],\n",
    "                                verbose=verbose\n",
    "                                )\n",
    "        models.append((model, lstm_args[i+1]))\n",
    "        print(f'{i+1}\\'s training. status=end {datetime.now()}')\n",
    "    print(f'completed to train all models. {datetime.now()}')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_predict(args: dict, lstm_args, verbose=False):\n",
    "    if 'test_id' not in args:\n",
    "        return\n",
    "    test_id = args['test_id']\n",
    "    version = args['version']\n",
    "    max_model_cnt = args['max_model_cnt']\n",
    "    write_to_db = args['write_to_db']\n",
    "    last = args['last']\n",
    "    db_file_name = args['db_file_name']\n",
    "    if 'mode2' not in args:\n",
    "        mode2 = \"sampling\"\n",
    "    else:\n",
    "        mode2 = args['mode2']\n",
    "    if 'gen_num' not in args:\n",
    "        gen_num = 5\n",
    "    else:\n",
    "        gen_num = args['gen_num']\n",
    "\n",
    "    if 'round_limit' not in args:\n",
    "        round_limit = 40\n",
    "    else:\n",
    "        round_limit = args['round_limit']\n",
    "    print(f'previous model_version={version}')\n",
    "    print(f'open model version file {datetime.now()}')\n",
    "    models = gen_multi_model(test_id=test_id,\n",
    "                             lstm_args=lstm_args,\n",
    "                             version=version,\n",
    "                             max_model_cnt=max_model_cnt,\n",
    "                             verbose=verbose\n",
    "                            )\n",
    "    print(f'check to create model({version}). {datetime.now()}')\n",
    "    predict_small(test_id=test_id,\n",
    "                  version=version,\n",
    "                  models=models,\n",
    "                  write_to_db=write_to_db,\n",
    "                  round_limit=round_limit,\n",
    "                  last=last,\n",
    "                  gen_num=gen_num,\n",
    "                  db_file_name=db_file_name,\n",
    "                  mode2=mode2,\n",
    "                  max_model_cnt=len(models),\n",
    "                  verbose=verbose\n",
    "                  )\n",
    "    save_model(test_id=test_id, version=version, models=models, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous model_version=20\n",
      "open model version file 2025-05-27 17:15:10.636234\n",
      "start to train all models. 2025-05-27 17:15:10.636253\n",
      "1's training. status=start 2025-05-27 17:15:10.636263\n",
      "lstm unit : 1, (45, 'selu')\n",
      "lstm unit : 2, (45, 'selu')\n",
      "lstm unit : 3, (45, 'sigmoid')\n",
      "lstm unit : 4, (45, 'elu')\n",
      "loss = binary_crossentropy\n",
      "verbose=0\n",
      "model=<Sequential name=sequential_120, built=True>\n",
      "num_epoch=50/<class 'int'>\n",
      "1's training. status=end 2025-05-27 17:16:08.222291\n",
      "2's training. status=start 2025-05-27 17:16:08.222389\n",
      "lstm unit : 1, (45, 'selu')\n",
      "lstm unit : 2, (45, 'selu')\n",
      "loss = binary_crossentropy\n",
      "verbose=0\n",
      "model=<Sequential name=sequential_121, built=True>\n",
      "num_epoch=20/<class 'int'>\n",
      "2's training. status=end 2025-05-27 17:16:27.635442\n",
      "3's training. status=start 2025-05-27 17:16:27.635538\n",
      "lstm unit : 1, (45, 'selu')\n",
      "lstm unit : 2, (45, 'selu')\n",
      "loss = binary_crossentropy\n",
      "verbose=0\n",
      "model=<Sequential name=sequential_122, built=True>\n",
      "num_epoch=50/<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# max_model_limit 정보만큼 반복적으로 매트릭을 생성합니다.\n",
    "#\n",
    "\n",
    "model_version_limit = 25\n",
    "max_model_cnt = 6 # 위에 정리 된 모델의 수\n",
    "for version in range(1, model_version_limit+1):\n",
    "    args = {'test_id': 'P4_4_1',\n",
    "        'version': version,\n",
    "        'max_model_cnt': max_model_cnt,\n",
    "        'write_to_db': True,\n",
    "        'last': last,\n",
    "        'mode2': mode2,\n",
    "        'gen_num': 5,\n",
    "        'round_limit': 40,\n",
    "        'db_file_name': \"./db/metrics.db\"\n",
    "       }\n",
    "    build_and_predict(args=args, lstm_args=lstm_args, verbose=False)\n",
    "    \n",
    "    clear_output_this_02 = True\n",
    "    \n",
    "    if clear_output_this_02:\n",
    "        from IPython.display import clear_output\n",
    "        clear_output() # Clears the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# active.version 정보에 따라서 매트릭을 생성합니다.\n",
    "#\n",
    "run_this_active_version=False\n",
    "if run_this_active_version:\n",
    "    model_version = 0\n",
    "    with open('active.version', 'r') as fd:\n",
    "        line = fd.read()\n",
    "        model_version = int(line)\n",
    "\n",
    "    args = {'test_id': 'P4_4_1',\n",
    "            'version': model_version,\n",
    "            'max_model_cnt': MAX_MODEL_CNT,\n",
    "            'write_to_db': True,\n",
    "            'last': last,\n",
    "            'mode2': mode2,\n",
    "            'gen_num': 5,\n",
    "            'round_limit': 40,\n",
    "            'db_file_name': \"./db/metrics.db\"\n",
    "           }\n",
    "        \n",
    "    build_and_predict(args=args, lstm_args=lstm_args, verbose=False) \n",
    "    model_version += 1\n",
    "    with open('active.version', 'w') as fd:\n",
    "        fd.write(f'{model_version}')\n",
    "\n",
    "    clear_output_this_01 = False\n",
    "    if clear_output_this_01:\n",
    "        from IPython.display import clear_output\n",
    "        clear_output() # Clears the output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run_this=False\n",
    "if run_this:\n",
    "    predict_small(test_id=test_id,\n",
    "                  version=model_version,\n",
    "                  models=models,\n",
    "                  write_to_db=write_to_db,\n",
    "                  round_limit=round_limit,\n",
    "                  last=last,\n",
    "                  gen_num=gen_num,\n",
    "                  db_file_name=db_file_name,\n",
    "                  mode2=mode2,\n",
    "                  max_model_cnt=len(models)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check to create model(16). 2025-05-26 18:29:12.048018\n"
     ]
    }
   ],
   "source": [
    "print(f'check to create model({model_version}). {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_model_info = False\n",
    "if view_model_info:\n",
    "    for i in range(MAX_MODEL_CNT):\n",
    "        print(models[i][0].model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all with trial : print predicted\n",
    "run = False\n",
    "if run:\n",
    "    write_json(trial = 3, matched_count=matched_cnts, matched_list=matched_list, append=True)\n",
    "    print_dict_list(title=\"matched_cnt\", datas=matched_cnts)\n",
    "    for i in range(MAX_MODEL_CNT):\n",
    "        print_list(title=f\"matched_list[{i+1}]\", datas=matched_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_db = True\n",
    "if write_to_db:\n",
    "    conn = conn_db('./db/metrics.db')\n",
    "    need_db_init = False\n",
    "    init_metric(conn, drop_table=need_db_init)\n",
    "else:\n",
    "    conn = None\n",
    "round_limit=40\n",
    "for round in range(round_limit+1):\n",
    "    matched_cnts_all = []\n",
    "    matched_list = []\n",
    "    date = str(datetime.now())\n",
    "    matched_cnts_small, matched_list_small = predict_small(round=round,\n",
    "                                                           trial=trial,\n",
    "                                                           last=last,\n",
    "                                                           models=models,\n",
    "                                                           max_model_cnt=MAX_MODEL_CNT,\n",
    "                                                           verbose=False,\n",
    "                                                           mode2=mode2\n",
    "                                                          )\n",
    "    #print_predict_small(matched_cnts_small, matched_list_small)\n",
    "    matched_cnts_all.append(matched_cnts_small)\n",
    "    matched_list.append(matched_list_small)\n",
    "    insert_metric(conn=conn,\n",
    "                  version=model_version,\n",
    "                  round=round,\n",
    "                  date=date,\n",
    "                  model_datas=matched_list)\n",
    "if write_to_db:\n",
    "    close_db(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = 1\n",
    "matched_cnts_all = []\n",
    "matched_list = []\n",
    "for t in range(1, trial+1):\n",
    "    matched_cnts_small, matched_list_small = predict_small(trial=t, \n",
    "                                                           last=last,\n",
    "                                                           models=models)\n",
    "    #print_predict_small(matched_cnts_small, matched_list_small)\n",
    "    matched_cnts_all.append(matched_cnts_small)\n",
    "    matched_list.append(matched_list_small)\n",
    "current +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all : save model\n",
    "for model in models:\n",
    "    if model is not None:\n",
    "        model[0].save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all : model info\n",
    "#################\n",
    "# model info\n",
    "#################\n",
    "for model in models:\n",
    "    if model is not None:\n",
    "        print(f'model:\\n\\t{model[0].model.summary()}')\n",
    "        print(f'layer:\\n\\t{model[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all : model test\n",
    "mode = \"back-test\"\n",
    "if mode == 'back-test':\n",
    "    for model in models:\n",
    "        if model is not None:\n",
    "            greed_prediction_number_set = model[0].predict_numbers(\"greed\", trial=1)\n",
    "            random_pred_set = model[0].predict_randomely(trial=1)\n",
    "            model[0].evaluate(greed_prediction_number_set)\n",
    "            print(\"---------Random baseline-------------\")\n",
    "            model[0].evaluate(random_pred_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f'{i} : {get_random_in_list(prediction_number_set, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'{i} : {get_random_in_list(prediction_number_set, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one : create model and training\n",
    "id = 6\n",
    "print(f'start to train. [id={id}]')\n",
    "models[6] = create_model_v1(id=6, dataset=dataset, epoch=10, verbose=True)\n",
    "print(f'model = {models[6]}')\n",
    "print(f'end to train. [id={id}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'{i} : {get_random_in_list(prediction_number_set2, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data(title='greed_predicted',\n",
    "           data_set=greed_prediction_number_set,\n",
    "           add_val=0, need_sort=True)\n",
    "print(f'greed_prediction_number_set\\n\\t\\t{greed_prediction_number_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pred_set = model2.predict_randomely(trial=1)\n",
    "print(f'prediction_number_set = {random_pred_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
