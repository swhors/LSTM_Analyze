{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5f1551-d867-454b-a16e-1e1e3cfdb76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 22:41:50.594050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step #01 [now = 2025-06-20 22:41:53.724955\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "print(f'Current Step #01 [now = {datetime.now()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b27c02-3545-4753-8e6b-ad6808735b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5743256f-980d-48ae-9438-c9a65f6bf4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(random_state, version, db_file_path):\n",
    "    query = f'select rounds, matched_cnts from rndforest where random_state={random_state} and version=\\\\\"{version}\\\\\"'\n",
    "    metrics = !echo {query} | sqlite3 {db_file_path}\n",
    "    metrics = metrics[0].split('|')\n",
    "    return metrics[0], metrics[1]\n",
    "\n",
    "\n",
    "def load_data(version=\"T_01_10\", db_file_path='../db/metrics.db', random_state=113789):\n",
    "    rounds_str, matched_cnts_str = load_metrics(random_state=random_state, db_file_path=db_file_path, version=version)\n",
    "    rounds = []\n",
    "    matched_cnts = []\n",
    "    for i in rounds_str.split(','):\n",
    "        rounds.append(int(i))\n",
    "    for i in matched_cnts_str.split(','):\n",
    "        matched_cnts.append(int(i))\n",
    "    rounds.reverse()\n",
    "    matched_cnts.reverse()\n",
    "    df = pd.DataFrame(zip(rounds, matched_cnts), columns=['rounds', 'matched_cnts'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d1c77a-7699-4aa2-aa3c-caeac7df4d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "time_steps = 10\n",
    "data_dim = 1\n",
    "num_samples = 100\n",
    "X = np.random.rand(num_samples, time_steps, data_dim)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29350a-7d72-4625-ad70-4171167f9ea4",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout,  Dense\n",
    "from tensorflow.keras.regularizers import l1_l2, l2\n",
    "\n",
    "total_words = 478\n",
    "max_sequence_len = 90\n",
    "model = Sequential()\n",
    "Layer1 = model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "Layer2 = model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "Layer3 = model.add(Dropout(.03))\n",
    "Layer4 = model.add(LSTM(20))\n",
    "Layer5 = model.add(Dense(total_words, \n",
    "    kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "    bias_regularizer=l2(1e-4),\n",
    "    activity_regularizer=l2(1e-5)))\n",
    "          # A Dense Layer including regularizers\n",
    "Layer6 = model.add(Dense(total_words, activation = 'softmax'))\n",
    "          \n",
    "# Pick an optimizer\n",
    "          \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d220ad-ae7b-47c8-a524-6228b6cac762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n",
      "\n",
      "Restored Data:\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "[[2.32   ]\n",
      " [3.     ]\n",
      " [4.33824]\n",
      " [4.38   ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1], [2], [3], [4], [5]])\n",
    "\n",
    "# Initialize and fit-transform the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Scaled Data:\")\n",
    "print(scaled_data)\n",
    "\n",
    "new_scaled = np.array([0.33, 0.5, 0.83456, 0.845])\n",
    "new_scaled = new_scaled.reshape(-1, 1)\n",
    "\n",
    "# Restore the original data using inverse_transform\n",
    "restored_data = scaler.inverse_transform(scaled_data)\n",
    "restored_data1 = scaler.inverse_transform(new_scaled)\n",
    "print(\"\\nRestored Data:\")\n",
    "print(restored_data)\n",
    "print(restored_data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a8bd3ee-f314-444f-bad6-816bbdf297ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work : 2025-06-21 00:57:50.558014\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class PrivateScaler:\n",
    "    def __init__(self, divider=6.0):\n",
    "        \"\"\"\" __init___ \"\"\"\n",
    "        self._divider = divider\n",
    "        self.p_max = 1\n",
    "        self.p_min = 0\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\" fit_transform \"\"\"\n",
    "        self.p_max = data.max() + 1\n",
    "        self._divider = self.p_max\n",
    "        scaled_data = np.array([[round(float(i[0])/self._divider, 9)] for i in data])\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\" inverse_transform \"\"\"\n",
    "        self.p_min = data.min()\n",
    "        if len(data) > 1:\n",
    "            restored = np.array([[((i[0] - self.p_min)*self._divider)] for i in data])\n",
    "            r_max = restored.max()\n",
    "            while r_max < 0.1:\n",
    "                restored *= 10\n",
    "                r_max = restored.max()\n",
    "        else:\n",
    "            restored = np.array([[((i[0])*self._divider)] for i in data])\n",
    "        return restored\n",
    "\n",
    "\n",
    "class ScalerType(Enum):\n",
    "    PRIVATE=0,\n",
    "    STANDARD=1,\n",
    "    MIN_MAX=2\n",
    "\n",
    "\n",
    "scaler_cls = {\n",
    "    ScalerType.PRIVATE: PrivateScaler,\n",
    "    ScalerType.STANDARD: StandardScaler,\n",
    "    ScalerType.MIN_MAX: MinMaxScaler # feature_range=(0,1)\n",
    "    }\n",
    "\n",
    "\n",
    "class DataScaling():\n",
    "    def __init__(self, scaler_type: ScalerType, *args: list):\n",
    "        self._scaler_type = scaler_type\n",
    "        self._scaler_cls = scaler_cls[scaler_type](*args)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self._scaler_cls.fit_transform(data)\n",
    "\n",
    "    def inverse_transform(self, datas: list):\n",
    "        inversed_data = []\n",
    "        if self._scaler_type == ScalerType.PRIVATE:\n",
    "            for data in datas:\n",
    "                if data[1] == True:\n",
    "                    inversed = self._scaler_cls.inverse_transform(data[0].reshape(-1, 1))\n",
    "                else:\n",
    "                    inversed = self._scaler_cls.inverse_transform(data[0])\n",
    "                inversed_data.append(inversed)\n",
    "        else:\n",
    "            cnt = 1\n",
    "            for data in datas:\n",
    "                print(f'cnt = {cnt}')\n",
    "                cnt += 1\n",
    "                if data[1] == True:\n",
    "                    inversed = self._scaler_cls.inverse_transform(data[0].reshape(-1, 1))\n",
    "                else:\n",
    "                    inversed = self._scaler_cls.inverse_transform(data[0])\n",
    "                inversed_data.append(inversed)\n",
    "        return inversed_data\n",
    "\n",
    "\n",
    "### 최종 결과 입니다.\n",
    "def analyze_v1(random_state,\n",
    "               version,\n",
    "               db_file_path,\n",
    "               predict_round=7,\n",
    "               predict_scale=10000000,\n",
    "               scaler_type=ScalerType.PRIVATE,\n",
    "               layer_count=3,\n",
    "               activation='sigmoid',\n",
    "               units=50,\n",
    "               draw_graph=True,\n",
    "               sequence_length=10,\n",
    "               verbose=0):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "    # 1. Prepare the Data\n",
    "    # Create sample time series data\n",
    "    df = load_data(version=version, db_file_path=db_file_path, random_state=random_state)\n",
    "    if verbose > 0:\n",
    "        print('df', df)\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #02 [now = {datetime.now()}]')\n",
    "\n",
    "    data = df['matched_cnts'].values.reshape(-1, 1)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = DataScaling(scaler_type)\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #04 [now = {datetime.now()}]')\n",
    "\n",
    "    # Define sequence length (timesteps)\n",
    "\n",
    "    # Create sequences for training\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - sequence_length):\n",
    "        X.append(scaled_data[i:i + sequence_length, 0])\n",
    "        y.append(scaled_data[i + sequence_length, 0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #06 [now = {datetime.now()}]')\n",
    "\n",
    "    # Reshape X for LSTM input (samples, timesteps, features)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #08 [now = {datetime.now()}]')\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    # train_size = int(len(X) * 0.8)\n",
    "    train_size = int(len(X) * 0.9)\n",
    "\n",
    "    X_train, X_test, X_last = X[:train_size], X[train_size:], np.array([[y[-1:]]])\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #10 [now = {datetime.now()}]')\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    # 2. Build the LSTM Model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "    # model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    for i in range(layer_count):\n",
    "        model.add(LSTM(units=units, return_sequences=True, activation=activation))\n",
    "    model.add(LSTM(units=units, activation=activation))\n",
    "    model.add(Dense(units=1)) # Output layer for predicting a single value\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #12 [now = {datetime.now()}]')\n",
    "\n",
    "    # 3. Compile and Train the Model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=0) # verbose=0 for silent training\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #14 [now = {datetime.now()}]')\n",
    "\n",
    "    # 4. Make Predictions\n",
    "    train_predict = model.predict(X_train, verbose=verbose)\n",
    "    test_predict = model.predict(X_test, verbose=verbose)\n",
    "    last_predict = model.predict(X_last, verbose=verbose)\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #16 [now = {datetime.now()}]')\n",
    "\n",
    "    # Inverse transform predictions to original scale\n",
    "    scaled_data1 = scaled_data\n",
    "    wanted_datas = [(train_predict, False),\n",
    "                    (test_predict, False),\n",
    "                    (last_predict, True),\n",
    "                    (y_train, True),\n",
    "                    (y_test, True),\n",
    "                    (scaled_data, False)]\n",
    "    inversed_datas = scaler.inverse_transform(wanted_datas)\n",
    "    train_predict = inversed_datas[0]\n",
    "    test_predict = inversed_datas[1]\n",
    "    last_predict = inversed_datas[2]\n",
    "    y_train_original = inversed_datas[3]\n",
    "    y_test_original = inversed_datas[4]\n",
    "    scaled_data = inversed_datas[5]\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #18 [now = {datetime.now()}]')\n",
    "\n",
    "    # 5. Evaluate the Model (Optional, but recommended)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_original, train_predict))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_original, test_predict))\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Current Step #20 [now = {datetime.now()}]')\n",
    "        print(f\"Train RMSE: {train_rmse}\")\n",
    "        print(f\"Test RMSE: {test_rmse}\")\n",
    "        print(f'random_state = {random_state}')\n",
    "        print(f'last_prediction = {last_predict}')\n",
    "        print(f'data_max_val = {scaled_data.max()}')\n",
    "    # You can also visualize the results\n",
    "    if draw_graph:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(scaled_data1, label='Original Data')\n",
    "        plt.plot(np.arange(sequence_length,\n",
    "                           sequence_length + len(train_predict)),\n",
    "                 train_predict,\n",
    "                 label='Train Predictions')\n",
    "        plt.plot(np.arange(sequence_length + len(train_predict),\n",
    "                           sequence_length + len(train_predict) + len(test_predict)),\n",
    "                 test_predict,\n",
    "                 label='Test Predictions')\n",
    "        plt.plot(np.arange(sequence_length + len(train_predict) + len(test_predict),\n",
    "                       sequence_length + len(train_predict) + len(test_predict) + len(last_predict)),\n",
    "                 last_predict,\n",
    "                 label='Last Predictions')\n",
    "        plt.xlabel(f'{random_state}\\'s Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return scaled_data.max(), last_predict, random_state\n",
    "\n",
    "\n",
    "print(f'Current work : {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164daf61-e518-4795-a3e6-3d265cb798cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"T_01_12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a25af793-0aaf-403c-b1be-790730f2571b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m activation = \u001b[33m'\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m units = \u001b[32m50\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43manalyze_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m           \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m           \u001b[49m\u001b[43mpredict_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m           \u001b[49m\u001b[43mdb_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m           \u001b[49m\u001b[43mscaler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m           \u001b[49m\u001b[43mlayer_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m           \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m           \u001b[49m\u001b[43munits\u001b[49m\u001b[43m=\u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m           \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36manalyze_v1\u001b[39m\u001b[34m(random_state, version, db_file_path, predict_round, predict_scale, scaler_type, layer_count, activation, units, draw_graph, sequence_length, verbose)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# 1. Prepare the Data\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Create sample time series data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m df = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose > \u001b[32m0\u001b[39m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m, df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(version, db_file_path, random_state)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m(version=\u001b[33m\"\u001b[39m\u001b[33mT_01_10\u001b[39m\u001b[33m\"\u001b[39m, db_file_path=\u001b[33m'\u001b[39m\u001b[33m../db/metrics.db\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m113789\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     rounds_str, matched_cnts_str = \u001b[43mload_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     rounds = []\n\u001b[32m     11\u001b[39m     matched_cnts = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_metrics\u001b[39m\u001b[34m(random_state, version, db_file_path)\u001b[39m\n\u001b[32m      2\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mselect rounds, matched_cnts from rndforest where random_state=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and version=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m metrics = get_ipython().getoutput(\u001b[33m'\u001b[39m\u001b[33mecho \u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[33m | sqlite3 \u001b[39m\u001b[38;5;132;01m{db_file_path}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m metrics = \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.split(\u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics[\u001b[32m0\u001b[39m], metrics[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "db_file_path = '../db/metrics.db'\n",
    "random_state = 113708\n",
    "predict_scale = 10000000\n",
    "predict_round = 8\n",
    "scaler_type=ScalerType.PRIVATE\n",
    "# layer information\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "\n",
    "analyze_v1(random_state=random_state,\n",
    "           version=version,\n",
    "           predict_round=predict_round,\n",
    "           db_file_path=db_file_path,\n",
    "           scaler_type=scaler_type,\n",
    "           layer_count=layer_count,\n",
    "           activation=activation,\n",
    "           units=units,\n",
    "           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b45fc1-e9c4-4778-963d-733a376061db",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file_path = '../db/metrics.db'\n",
    "random_state = 113799\n",
    "# predict_scale = 10000000\n",
    "predict_scale = 1000000\n",
    "predict_round = 7\n",
    "scaler_type=ScalerType.PRIVATE\n",
    "# layer information\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "analyze_v1(random_state=random_state, version=version, db_file_path=db_file_path, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737f8ad-8d66-43ec-9115-7372dcddf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 최종 결과 입니다. (113789)\n",
    "db_file_path = '../db/metrics.db'\n",
    "random_state = 113789\n",
    "predict_scale = 10000000\n",
    "predict_round = 7\n",
    "scaler_type=ScalerType.PRIVATE\n",
    "# layer information\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "analyze_v1(random_state=random_state,\n",
    "           version=version,\n",
    "           predict_round=predict_round,\n",
    "           db_file_path=db_file_path,\n",
    "           scaler_type=scaler_type,\n",
    "           layer_count=layer_count,\n",
    "           activation=activation,\n",
    "           units=units,\n",
    "           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a3b6c-afb0-4c79-a9a5-7236486c4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 최종 결과 입니다. (113789)\n",
    "db_file_path = '../db/metrics.db'\n",
    "random_state = 113700\n",
    "predict_scale = 10000000\n",
    "predict_round = 7\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "scaler_type=ScalerType.PRIVATE\n",
    "analyze_v1(random_state=random_state,\n",
    "           version=version,\n",
    "           predict_round=predict_round,\n",
    "           db_file_path=db_file_path,\n",
    "           scaler_type=scaler_type,\n",
    "           layer_count=layer_count,\n",
    "           activation=activation,\n",
    "           units=units,\n",
    "           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d17dc0-79bd-4bd5-8f27-a0364566b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file_path = '../db/metrics.db'\n",
    "random_state = 113705\n",
    "# predict_scale = 10000000\n",
    "predict_scale = 1000000\n",
    "predict_round = 7\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "scaler_type=ScalerType.PRIVATE\n",
    "analyze_v1(random_state=random_state,\n",
    "           version=version,\n",
    "           predict_round=predict_round,\n",
    "           db_file_path=db_file_path,\n",
    "           scaler_type=scaler_type,\n",
    "           layer_count=layer_count,\n",
    "           activation=activation,\n",
    "           units=units,\n",
    "           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0eb667-3953-48ff-a1da-fd7fa4ec8412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step : [now 2025-06-20 22:43:02.880812]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "version TEXT, random_state INTEGER, possibility FLOAT, max INTEGER\n",
    "\"\"\"\n",
    "def insert_possi(version, max, possi, random_state):\n",
    "    table_name='rndforest_possi'\n",
    "    db_file_path='../db/metrics.db'\n",
    "    sql = f'insert into {table_name} (version, random_state, possibility, max) values(\\\\\"{version}\\\\\", {random_state}, {possi}, {max});'\n",
    "    !echo \"{sql}\" | sqlite3 {db_file_path}\n",
    "\n",
    "print(f'Current Step : [now {datetime.now()}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "370ed0de-6de0-49bd-bfcb-08fe5c22c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possibility(random_states, db_file_path, layer_count, activation, units, scaler_type, sequence_length=10):\n",
    "    random_state_begin = 3700\n",
    "    # predict_scale = 10000000\n",
    "    predict_scale = 1000000\n",
    "    predict_round = 7\n",
    "    results = []\n",
    "    for random_state in tqdm(random_states):\n",
    "        scaled_data_max, last_predict, random_state = analyze_v1(random_state=random_state,\n",
    "                                                                 version=version,\n",
    "                                                                 predict_round=predict_round,\n",
    "                                                                 db_file_path=db_file_path,\n",
    "                                                                 scaler_type=scaler_type,\n",
    "                                                                 layer_count=layer_count,\n",
    "                                                                 activation=activation,\n",
    "                                                                 units=units,\n",
    "                                                                 draw_graph=False,\n",
    "                                                                 sequence_length=sequence_length,\n",
    "                                                                 verbose=0)\n",
    "        results.append((scaled_data_max, last_predict, random_state))\n",
    "        if last_predict > 1.7:\n",
    "            print(scaled_data_max, last_predict, random_state)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa734f53-6096-4bee-addd-05c5ac925edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [11:18<00:00, 135.68s/it]\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "#113780에서 5씩 증가\n",
    "\n",
    "db_file_path = '../db/metrics.db'\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "gap = 5\n",
    "scaler_type = ScalerType.PRIVATE\n",
    "random_states = range(random_state_begin, random_state_begin+gap)\n",
    "results = get_possibility(random_states=random_states,\n",
    "                          db_file_path=db_file_path,\n",
    "                          layer_count=layer_count,\n",
    "                          activation=activation,\n",
    "                          units=units,\n",
    "                          scaler_type=scaler_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7005e-e85a-480e-b570-a610865a1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3731, 3786, 3702, 3715, 3734, 3753, 3761, 3770]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "#113780에서 5씩 증가\n",
    "\n",
    "db_file_path = '../db/metrics.db'\n",
    "layer_count = 3\n",
    "activation = 'sigmoid'\n",
    "units = 50\n",
    "scaler_type=ScalerType.PRIVATE\n",
    "rows = \"\\\n",
    "('74692', '3705', 10)\\\n",
    "('74702', '3715', 10)\\\n",
    "('74704', '3717', 10)\\\n",
    "('74712', '3725', 10)\\\n",
    "('74721', '3734', 10)\\\n",
    "('74740', '3753', 10)\\\n",
    "('74743', '3756', 10)\\\n",
    "('74748', '3761', 10)\\\n",
    "('74757', '3770', 10)\\\n",
    "('74775', '3788', 10)\\\n",
    "('74777', '3790', 10)\\\n",
    "('74779', '3792', 10)\\\n",
    "('74698', '3711', 11)\\\n",
    "('74715', '3728', 11)\\\n",
    "('74716', '3729', 11)\\\n",
    "('74730', '3743', 11)\\\n",
    "('74746', '3759', 11)\\\n",
    "('74778', '3791', 11)\\\n",
    "('74781', '3794', 11)\\\n",
    "('74722', '3735', 12)\\\n",
    "('74766', '3779', 12)\\\n",
    "('74767', '3780', 12)\\\n",
    "('74782', '3795', 12)\\\n",
    "('74696', '3709', 13)\\\n",
    "('74699', '3712', 13)\\\n",
    "('74711', '3724', 13)\\\n",
    "('74723', '3736', 13)\\\n",
    "('74729', '3742', 13)\\\n",
    "('74720', '3733', 14)\\\n",
    "\"\n",
    "# ('74689', '3702', 10)\\\n",
    "\n",
    "random_states = []\n",
    "data_rows_str = rows.replace(\"(\",\"\")\n",
    "data_rows = data_rows_str.split(\")\")\n",
    "for row in data_rows:\n",
    "    if len(row) > 0:\n",
    "        row = row.replace(\"\\'\",\"\")\n",
    "        cols = row.split(\", \")\n",
    "        if len(cols) > 0:\n",
    "            random_states.append(int(cols[1]))\n",
    "random_states=[3731,3786,3702,3715,3734,3753,3761,3770]\n",
    "print(random_states)\n",
    "results = get_possibility(random_states=random_states,\n",
    "                          db_file_path=db_file_path,\n",
    "                          layer_count=layer_count,\n",
    "                          activation=activation,\n",
    "                          units=units,\n",
    "                          sequence_length=30,\n",
    "                          scaler_type=scaler_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c6962a8-edae-4623-b5da-9cc6e3c2e353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(3.0), array([[0.51065975]]), 3731)\n",
      "(np.float64(4.0), array([[0.03436218]]), 3786)\n",
      "(np.float64(4.999999998), array([[0.98191944]]), 3702)\n",
      "(np.float64(3.0), array([[1.15016282]]), 3715)\n",
      "(np.float64(3.0), array([[1.67712224]]), 3734)\n",
      "(np.float64(4.0), array([[-0.16663458]]), 3753)\n",
      "(np.float64(4.0), array([[0.22691065]]), 3761)\n",
      "(np.float64(3.0), array([[1.05514777]]), 3770)\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af483197-a567-4eb6-87b9-75e357829419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(3.0), array([[0.65072489]]), 3731)\n",
      "(np.float64(4.0), array([[0.50776754]]), 3786)\n",
      "(np.float64(4.999999998), array([[0.55112202]]), 3702)\n",
      "(np.float64(3.0), array([[0.77709711]]), 3715)\n",
      "(np.float64(3.0), array([[1.09201658]]), 3734)\n",
      "(np.float64(4.0), array([[1.04803503]]), 3753)\n",
      "(np.float64(4.0), array([[0.75670734]]), 3761)\n",
      "(np.float64(3.0), array([[0.96134448]]), 3770)\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)\n",
    "    insert_possi(version=version, max=result[0], possi=result[1][0][0], random_state=result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066afd86-eee7-4e51-ae84-7ba1ccdfbe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ace55a-7dab-46c1-8159-449059c56203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actual_numbers = [\n",
    "    [1175, 3, 4, 6, 8, 32, 42],\n",
    "    [1174, 8, 11, 14, 17, 36, 39],\n",
    "    [1173, 1, 5, 18, 20, 30, 35],\n",
    "    [1172, 7, 9, 24, 40, 42, 44],\n",
    "    [1171, 3, 6, 7, 11, 12, 17],\n",
    "    [1170, 3, 13, 28, 34, 38, 42],\n",
    "    [1169, 5, 12, 24, 26, 39, 42],\n",
    "    [1168, 9, 21, 24, 30, 33, 37],\n",
    "    [1167, 8, 23, 31, 35, 39, 40],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea48970-cf21-4deb-9c37-fab262a90c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83c041-b9cb-41f7-8943-060a4961492a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
